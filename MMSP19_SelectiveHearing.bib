%% In order of citation in the paper %%

% [1]
@ARTICLE{Valimaki2015,
author={V. {Valimaki} and A. {Franck} and J. {Ramo} and H. {Gamper} and L. {Savioja}},
journal={IEEE Signal Processing Magazine},
title={Assisted Listening Using a Headset: Enhancing audio perception in real, augmented, and virtual environments},
year={2015},
volume={32},
number={2},
pages={92-99},
keywords={audio signal processing;headphones;assisted listening;headset;audio perception;virtual environments;real environments;headphones;analytic listening;music production;music players;mobile phones;active noise control;ANC;equalization techniques;Acoustic signal processing;Acoustic noise;Interpolation;Headphones;Magnetic heads;Convolution;Noise abatement;Music},
doi={10.1109/MSP.2014.2369191},
ISSN={1053-5888},
month={March},}

% [2]
@conference{brandenburg2018,
title = {Plausible Augmentation of Auditory Scenes Using Dynamic Binaural Synthesis for Personalized Auditory Realities},
author = {Brandenburg, Karlheinz and Cano, Estefanía and Klein, Florian and K\"{o}llmer, Thomas and Lukashevich, Hanna and Neidhardt, Annika and Sloma, Ulrike and Werner, Stephan},
booktitle = {Proc. of AES International Conference on Audio for Virtual and Augmented Reality},
month = {Aug},
year = {2018},
}

% [3]
@article{argentieri,
title = "A survey on sound source localization in robotics: From binaural to array processing methods",
journal = "Computer Speech & Language",
volume = "34",
number = "1",
pages = "87-112",
year = "2015",
issn = "0885-2308",
doi = "https://doi.org/10.1016/j.csl.2015.03.003",
author = "S. Argentieri and P. Danès and P. Souères",
keywords = "Robot audition, Source localization, Binaural audition, Array processing",
abstract = "This paper attempts to provide a state-of-the-art of sound source localization in robotics. Noticeably, this context raises original constraints—e.g. embeddability, real time, broadband environments, noise and reverberation—which are seldom simultaneously taken into account in acoustics or signal processing. A comprehensive review is proposed of recent robotics achievements, be they binaural or rooted in array processing techniques. The connections are highlighted with the underlying theory as well as with elements of physiology and neurology of human hearing."
}

% [4]      
@article{Fitzgerald2016, 
author={D. FitzGerald and A. Liutkus and R. Badeau}, 
journal={IEEE/ACM Trans. on Audio, Speech, and Language Processing}, 
title={Projection-Based Demixing of Spatial Audio}, 
year={2016}, 
volume={24}, 
number={9}, 
pages={1560-1572}, 
doi={10.1109/TASLP.2016.2570945}, 
ISSN={2329-9290} 
}

% [5]
@ARTICLE{Cano2019, 
author={E. {Cano} and D. {FitzGerald} and A. {Liutkus} and M. D. {Plumbley} and F. {St{\"o}ter}}, 
journal={IEEE Signal Processing Magazine}, 
title={Musical Source Separation: An Introduction}, 
year={2019}, 
volume={36}, 
number={1}, 
pages={31-40}, 
keywords={acoustic signal processing;audio signal processing;music;source separation;sound channels;musical source separation;musical audio object;musical instrument;5.1-channel surround sound system;two-channel stereo recording;vocals;compact discs;Music;Harmonic analysis;Multiple signal classification;Instruments;Spectrogram;Power harmonic filters;Source separation}, 
doi={10.1109/MSP.2018.2874719}, 
ISSN={1053-5888}, 
month={Jan},}

%  [6]
@ARTICLE{Gannot2017,
author={S. {Gannot} and E. {Vincent} and S. {Markovich-Golan} and A. {Ozerov}},
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
title={A Consolidated Perspective on Multimicrophone Speech Enhancement and Source Separation},
year={2017},
volume={25},
number={4},
pages={692-730},
keywords={audio signal processing;blind source separation;microphone arrays;parameter estimation;spatial filters;speaker recognition;speech enhancement;transient response;multimicrophone speech enhancement;audio signal processing;consolidated perspective;audio signal processing;commercial applications;mobile phones;conference call systems;hands-free systems;hearing aids;noise-robust automatic speech recognition;speaker recognition;multichannel interfaces;single-channel interfaces;blind source separation;microphone array processing;acoustic impulse response model;spatial filter design criterion;parameter estimation algorithm;optional postfiltering;software resources;data resources;transverse axes;convergent paths;Microphones;Speech enhancement;Speech;Acoustics;Source separation;Arrays;Array processing;beamforming;expectation-maximization;independent component analysis;multichannel;postfiltering;sparse component analysis;wiener filter},
doi={10.1109/TASLP.2016.2647702},
ISSN={2329-9290},
month={April},}

%  [7]
@INPROCEEDINGS{Cano2017, 
author={E. {Cano} and J. {Nowak} and S. {Grollmisch}}, 
booktitle={Proc. of 25th European Signal Processing Conference (EUSIPCO)}, 
title={Exploring sound source separation for acoustic condition monitoring in industrial scenarios}, 
year={2017}, 
volume={}, 
number={}, 
pages={2264-2268}, 
keywords={acoustic signal processing;blind source separation;condition monitoring;fault diagnosis;learning (artificial intelligence);mechanical engineering computing;source separation;sound source separation;SSS;industrial acoustic condition monitoring scenarios;simulated measured data;interfering signal;signal-to-interference ratios;machine learning approach;machine sounds;Signal to noise ratio;Microphones;Source separation;Acoustics;Condition monitoring;Interference;Signal processing algorithms}, 
doi={10.23919/EUSIPCO.2017.8081613}, 
ISSN={2076-1465}, 
month={Aug},}

%  [8]
@ARTICLE{Gerkamnn2015,
author={T. {Gerkmann} and M. {Krawczyk-Becker} and J. {Le Roux}},
journal={IEEE Signal Processing Magazine},
title={Phase Processing for Single-Channel Speech Enhancement: History and recent advances},
year={2015},
volume={32},
number={2},
pages={55-66},
keywords={array signal processing;discrete Fourier transforms;hearing aids;speech enhancement;phase processing;single-channel speech enhancement;assisted listening devices;speech communication devices;hearing aids;cochlear implants;mobile telephones;beamforming;spatial diversity;corrupted speech;time-frequency representation;short-time discrete Fourier transform domain;Cochlear implants;Spectrogram;Speech enhancement;Time-domain analysis;Acoustic signal processing;Noise measurement;Time-frequency analysis;Implants;Assistive devices;Acoustic noise},
doi={10.1109/MSP.2014.2369251},
ISSN={1053-5888},
month={March},}

% [9]
@book{vincent2018audio,
  title={Audio Source Separation and Speech Enhancement},
  author={Vincent, E. and Virtanen, T. and Gannot, S.},
  isbn={9781119279914},
  lccn={2018013163},
  year={2018},
  publisher={Wiley}
}

% [10]
@inproceedings{Matz2015,
title={New Sonorities for Early Jazz Recordings Using Sound Source Separation and Automatic Mixing Tools},
author={Daniel Matz and Estefan{\'i}a Cano and Jakob Abe{\ss}er},
booktitle= {Proc. of the 16th International Society for Music Information Retrieval Conference},
year         = 2015,
pages        = {749-755},
publisher    = {ISMIR},
address      = {Malaga, Spain},
month        = oct,
venue        = {Malaga, Spain}
}


% [11]
@ARTICLE{Kuo99, 
author={S. M. {Kuo} and D. R. {Morgan}}, 
journal={Proceedings of the IEEE}, 
title={Active noise control: a tutorial review}, 
year={1999}, 
volume={87}, 
number={6}, 
pages={943-973}, 
keywords={active noise control;acoustic signal processing;adaptive signal processing;active noise control;antinoise wave;secondary source;online secondary path model;electronic system;noise cancellation;adaptive signal processing;digital signal processing;multiple-channel ANC algorithm;broad-band feedforward control;narrow-band feedforward control;adaptive feedback control;single-channel ANC algorithm;lattice algorithm;frequency domain algorithm;subband algorithm;recursive least squares algorithm;Tutorial;Active noise reduction;Signal processing algorithms;Noise cancellation;Digital signal processing;Adaptive algorithm;Electrical equipment industry;Manufacturing industries;Consumer products;Adaptive signal processing}, 
doi={10.1109/5.763310}, 
ISSN={0018-9219}, 
month={June},}

% [12]
@INPROCEEDINGS{McPherson2016, 
author={A.P. {McPherson} and RH. {Jack} and G. {Moro}}, 
booktitle={Proceedings of the International Conference on New Interfaces for Musical Expression}, 
title={Action-Sound Latency: Are Our Tools Fast Enough?}, 
year={2016}, 
month={July}}

% [13]
@ARTICLE{Rottondi2016,
author={C. {Rottondi} and C. {Chafe} and C. {Allocchio} and A. {Sarti}},
journal={IEEE Access},
title={An Overview on Networked Music Performance Technologies},
year={2016},
volume={4},
number={},
pages={8823-8843},
keywords={audio systems;Internet;psycho-perceptual studies;telecommunication network;Internet;networked music performance;Music;Audio systems;Performance evaluatoin;Internet;Computer generated music;Audio visual systems;Music;audio systems;audio-visual systems;networked music performance;network latency},
doi={10.1109/ACCESS.2016.2628440},
ISSN={2169-3536},
month={},}

% [14]
@INPROCEEDINGS{Liebich2018, 
author={S. {Liebich} and J. {Fabry} and P. {Jax} and P. {Vary}}, 
booktitle={Speech Communication; 13th ITG-Symposium}, 
title={Signal Processing Challenges for Active Noise Cancellation Headphones}, 
year={2018}, 
month={Oct},}

% [15]
@INPROCEEDINGS{Cano2018,
author={E. {Cano} and J. {Liebetrau} and D. {Fitzgerald} and K. {Brandenburg}},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={The Dimensions of Perceptual Quality of Sound Source Separation},
year={2018},
volume={},
number={},
pages={601-605},
keywords={acoustic signal processing;distortion;source separation;perceptual quality;sound source separation algorithms;listening test;sound separation quality evaluation;quality metrics;target distortion;artifact distortion;separation quality;free-choice profiling;repertory grid technique;two-dimensional perceptual space;Vocabulary;Measurement;Distortion;Source separation;Interference;Correlation;Harmonic analysis;Sound Source Separation;Quality Perception;Quality Metrics;Listening Tests;Repertory Grid},
doi={10.1109/ICASSP.2018.8462325},
ISSN={2379-190X},
month={April},}


% [16]
@INPROCEEDINGS{DelgadoPM2019,
author={P. M. {Delgado} and J. {Herre}},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Objective Assessment of Spatial Audio Quality Using Directional Loudness Maps},
year={2019},
volume={},
number={},
pages={621-625},
keywords={Spatial Audio;Objective Quality Assessment;PEAQ;Panning Index},
doi={10.1109/ICASSP.2019.8683810},
ISSN={2379-190X},
month={May},}

% [17]
@ARTICLE{Taal2011,
author={C. H. {Taal} and R. C. {Hendriks} and R. {Heusdens} and J. {Jensen}},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
title={{An Algorithm for Intelligibility Prediction of Time Frequency Weighted Noisy Speech}},
year={2011},
volume={19},
number={7},
pages={2125-2136},
keywords={speech enhancement;speech intelligibility;intelligibility prediction;time-frequency weighted noisy speech;noise-reduction algorithm;objective machine-driven intelligibility measure;speech intelligibility;noisy unprocessed speech;short-time objective intelligibility measure;objective intelligibility model;Speech;Noise measurement;Correlation;Speech processing;Signal to noise ratio;Time frequency analysis;Noise reduction;objective measure;speech enhancement;speech intelligibility prediction},
doi={10.1109/TASL.2011.2114881},
ISSN={1558-7916},
month={Sep.},}

% [18]
@book{PlumbleyDcase,
title = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
author = "Mark D. Plumbley and Christian Kroos and Juan P. Bello and Gaël Richard and Daniel P.W. Ellis and Annamaria Mesaros",
year = "2018",
publisher = "Tampere University of Technology. Laboratory of Signal Processing",
}


% [19]
@inproceedings{Serizel2018,
author = "Serizel, Romain and Turpault, Nicolas and Eghbal-Zadeh, Hamid and Shah, Ankit Parag",
abstract = "This paper presents DCASE 2018 task 4. The task evaluates systems for the large-scale detection of sound events using weakly labeled data (without time boundaries). The target of the systems is to provide not only the event class but also the event time boundaries given that multiple events can be present in an audio recording. Another challenge of the task is to explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly labeled training set to improve system performance. The data are Youtube video excerpts from domestic context which have many applications such as ambient assisted living. The domain was chosen due to the scientific challenges (wide variety of sounds, time-localized events. . . ) and potential applications.",
month = "November",
pages = "19--23",
year = "2018",
keywords = "Sound event detection, Large scale, Weakly labeled data, Semi-supervised learning",
title = "Large-scale weakly labeled semi-supervised sound event detection in domestic environments",
booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)"
}

% [20]
@techreport{jiakai2018mean,
author = "JiaKai, Lu",
institution = "DCASE2018 Challenge",
title = "Mean Teacher Convolution System for DCASE 2018 Task 4",
journal = "Detection and Classification of Acoustic Scenes and Events",
month = "September",
year = "2018",
abstract = "In this paper, we present our neural network for the DCASE 2018 challenge’s Task 4 (Large-scale weakly labeled semi-supervised sound event detection in domestic environments). This task evaluates systems for the large-scale detection of sound events using weakly labeled data, and explore the possibility to exploit a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance to doing audio tagging and sound event detection. We propose a mean-teacher model with context-gating convolutional neural network (CNN) and recurrent neural network (RNN) to maximize the use of unlabeled in-domain dataset."
}

% [21]
@INPROCEEDINGS{Parascandolo2016, 
author={G. {Parascandolo} and H. {Huttunen} and T. {Virtanen}}, 
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Recurrent neural networks for polyphonic sound event detection in real life recordings}, 
year={2016}, 
volume={}, 
number={}, 
pages={6440-6444}, 
keywords={audio signal processing;recurrent neural nets;recurrent neural networks;polyphonic sound event detection;real life recordings;bidirectional long short term memory;single multilabel BLSTM RNN;acoustic features;mixture signal;binary activity indicators;data augmentation techniques;Context;Recurrent neural networks;Feature extraction;Training;Neurons;Event detection;Recurrent neural network;bidirectional LSTM;deep learning;polyphonic sound event detection}, 
doi={10.1109/ICASSP.2016.7472917}, 
ISSN={2379-190X}, 
month={March},}

% [22]
@INPROCEEDINGS{Cakir:End2end:IJCNN-2018,
author={E. {\c{C}akir} and T. {Virtanen}},
booktitle={Proc. of International Joint Conference on Neural Networks (IJCNN)},
title={End-to-End Polyphonic Sound Event Detection Using Convolutional Recurrent Neural Networks with Learned Time-Frequency Representation Input},
year={2018},
volume={},
number={},
pages={1-7},
ISSN={2161-4407},
month={July},}

% [23]
@inproceedings{Xu:2018:WeaklyLabeledEventDetection:ICASSP,
address = {Calgary, AB, Canada},
author = {Xu, Yong and Kong, Qiuqiang and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2018.8461975},
title = {{Large-Scale Weakly Supervised Audio Classification Using Gated Convolutional Neural Network}},
pages={121--125},
year = {2018}
}

% [24]
@ARTICLE{Frenay2014, 
author={B. {Frenay} and M. {Verleysen}}, 
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title={Classification in the Presence of Label Noise: A Survey}, 
year={2014}, 
volume={25}, 
number={5}, 
pages={845-869}, 
keywords={learning (artificial intelligence);pattern classification;label noise classification;potential negative consequences;label noise-robust algorithm;label noise-tolerant algorithms;label noise cleansing algorithm;machine learning;Noise;Labeling;Training;Noise measurement;Taxonomy;Reliability;Context;Class noise;classification;label noise;mislabeling;robust methods;survey.;Class noise;classification;label noise;mislabeling;robust methods;survey}, 
doi={10.1109/TNNLS.2013.2292894}, 
ISSN={2162-237X}, 
month={May},}

% [25]
@inproceedings{Fonseca2019learning,
author = "Fonseca, Eduardo and Plakal, Manoj and Ellis, Daniel P. W. and Font, Frederic and Favory, Xavier and Serra, Xavier",
title = "Learning Sound Event Classifiers from Web Audio with Noisy Labels",
booktitle = "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
year = "2019",
abstract = "As sound event classification moves towards larger datasets, issues of label noise become inevitable. Web sites can supply large volumes of user-contributed audio and metadata, but inferring labels from this metadata introduces errors due to unreliable inputs, and limitations in the mapping. There is, however, little research into the impact of these errors. To foster the investigation of label noise in sound event classification we present FSDnoisy18k, a dataset containing 42.5 hours of audio across 20 sound classes, including a small amount of manually-labeled data and a larger quantity of real-world noisy data. We characterize the label noise empirically, and provide a CNN baseline system. Experiments suggest that training with large amounts of noisy data can outperform training with smaller amounts of carefully-labeled data. We also show that noise-robust loss functions can be effective in improving performance in presence of corrupted labels.",
address = "Brighton, UK"
}

% [26]
@inproceedings{Dorfer2018,
author = "Dorfer, Matthias and Widmer, Gerhard",
title = "Training General-Purpose Audio Tagging Networks with Noisy Labels and Iterative Self-Verification",
booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
year = "2018",
address = "Surrey, UK"
}

%  [27]
@article{Adavanne2018_JSTSP,
author = "Adavanne, Sharath and Politis, Archontis and Nikunen, Joonas and Virtanen, Tuomas",
doi = "10.1109/JSTSP.2018.2885636",
title = "Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks",
journal = "IEEE Journal of Selected Topics in Signal Processing",
issn = "1932-4553",
pages = "1-1",
year = "2018",
keywords = "Direction-of-arrival estimation;Estimation;Task analysis;Azimuth;Microphone arrays;Recurrent neural networks;Sound event detection;direction of arrival estimation;convolutional recurrent neural network",
abstract = "In this paper, we propose a convolutional recurrent neural network for joint sound event localization and detection (SELD) of multiple overlapping sound events in three-dimensional (3D) space. The proposed network takes a sequence of consecutive spectrogram time-frames as input and maps it to two outputs in parallel. As the first output, the sound event detection (SED) is performed as a multi-label classification task on each time-frame producing temporal activity for all the sound event classes. As the second output, localization is performed by estimating the 3D Cartesian coordinates of the direction-of-arrival (DOA) for each sound event class using multi-output regression. The proposed method is able to associate multiple DOAs with respective sound event labels and further track this association with respect to time. The proposed method uses separately the phase and magnitude component of the spectrogram calculated on each audio channel as the feature, thereby avoiding any method- and array-specific feature extraction. The method is evaluated on five Ambisonic and two circular array format datasets with different overlapping sound events in anechoic, reverberant and real-life scenarios. The proposed method is compared with two SED, three DOA estimation, and one SELD baselines. The results show that the proposed method is generic and applicable to any array structures, robust to unseen DOA values, reverberation, and low SNR scenarios. The proposed method achieved a consistently higher recall of the estimated number of DOAs across datasets in comparison to the best baseline. Additionally, this recall was observed to be significantly better than the best baseline method for a higher number of overlapping sound events."
}

% [28]
@inproceedings{Jung2018,
author = {Jung, Youngmoon and Kim, Younggwan and Choi, Yeunju and Kim, Hoirin},
year = {2018},
booktitle = {Proc. of Interspeech},
month = {September},
pages = {1210-1214},
title = {Joint Learning Using Denoising Variational Autoencoders for Voice Activity Detection},
doi = {10.21437/Interspeech.2018-1151}
}

% [29]
@INPROCEEDINGS{Eyben2013,
author={F. {Eyben} and F. {Weninger} and S. {Squartini} and B. {Schuller}},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing},
title={Real-life voice activity detection with {LSTM} Recurrent Neural Networks and an application to Hollywood movies},
year={2013},
pages={483-487},
keywords={error statistics;recurrent neural nets;signal detection;speech processing;speech recognition;state of the art reference algorithm;equal error rate;real long term recording;TIMIT and Buckeye corpora;spontaneous speech;read speech;RASTA-PLP frontend feature;long short-term memory recurrent neural networks;Hollywood movies;LSTM recurrent neural networks;real life voice activity detection;Speech;Noise;Motion pictures;Training;Noise measurement;Context;Hidden Markov models;Voice Activity Detection;Speech Detection;Neural Networks;Long Short-Term Memory},
doi={10.1109/ICASSP.2013.6637694},
ISSN={1520-6149},
month={May},}

% [30]
@inproceedings{ZazoCandil2016FeatureLW,
title={Feature Learning with Raw-Waveform {CLDNNs} for Voice Activity Detection},
author={Rub{\'e}n Zazo-Candil and Tara N. Sainath and Gabor Simko and Carolina Parada},
booktitle={Proc. of Interspeech},
year={2016}
}

% [31]
@INPROCEEDINGS{McLaren2015,
author={M. {McLaren} and Y. {Lei} and L. {Ferrer}},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Advances in deep neural network approaches to speaker recognition},
year={2015},
volume={},
number={},
pages={4814-4818},
keywords={feature extraction;neural nets;speaker recognition;deep neural network;speaker identification;microphone speech;feature extraction;bottleneck features;NIST 2012 speaker recognition evaluation;Speech;Feature extraction;Microphones;Speaker recognition;Neural networks;NIST;Noise;Deep neural networks;bottleneck features;normalization;channel mismatch;speaker recognition},
doi={10.1109/ICASSP.2015.7178885},
ISSN={1520-6149},
month={April},}

% [32]
@INPROCEEDINGS{Snyder2018,
author={D. {Snyder} and D. {Garcia-Romero} and G. {Sell} and D. {Povey} and S. {Khudanpur}},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={X-Vectors: Robust {DNN} Embeddings for Speaker Recognition},
year={2018},
pages={5329-5333},
keywords={learning (artificial intelligence);neural nets;speaker recognition;x-vectors;robust DNN embeddings;speaker recognition;data augmentation;deep neural network embeddings;fixed-dimensional embeddings;training data;i-vector baselines;i-vector extractor;x-vector DNN;supervised training;reverberation data;variable-length utterances;large-scale training datasets;Speakers in the Wild;NIST SRE 2016 Cantonese;PLDA classifier;Acoustics;Training;Feature extraction;Speaker recognition;Training data;Neural networks;NIST;speaker recognition;deep neural networks;data augmentation;x-vectors},
doi={10.1109/ICASSP.2018.8461375},
ISSN={2379-190X},
month={April},}
@conference{stitt2016,
title = {The Influence of Head Tracking Latency on Binaural Rendering in Simple and Complex Sound Scenes},
author = {Stitt, Peter and Hendrickx, Etienne and Messonnier, Jean-Christophe and Katz, Brian},
booktitle = {Audio Engineering Society Convention 140},
month = {May},
year = {2016},
url = {http://www.aes.org/e-lib/browse.cfm?elib=18289}
}

% [33]
@inproceedings{McLaren2018HowTT,
title={How to train your speaker embeddings extractor},
author={Mitchell McLaren and Diego Cast{\'a}n and Mahesh Kumar Nandwana and Luciana Ferrer and Emre Yilmaz},
booktitle={Odyssey},
year={2018}
}


% [34]
@inproceedings{Sadjadi2016,
author={Seyed Omid Sadjadi and Jason W. Pelecanos and Sriram Ganapathy},
title={The {IBM} Speaker Recognition System: Recent Advances and Error Analysis},
year=2016,
booktitle={Proc. of Interspeech},
doi={10.21437/Interspeech.2016-1159},
pages={3633--3637}
}

%  [35]
@ARTICLE{Han2017,
author={Y. {Han} and J. {Kim} and K. {Lee}},
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
title={Deep Convolutional Neural Networks for Predominant Instrument Recognition in Polyphonic Music},
year={2017},
volume={25},
number={1},
pages={208-221},
keywords={audio signal processing;music;neural nets;source separation;support vector machines;music information retrieval;real-world polyphonic music;fixed-length music excerpt;audio signal;sliding window;test audio;temporally local class-wise max-pooling;activation function;identification threshold;analysis window size;source separation;support vector machine;spectral feature exploitation;predominant instrument recognition;deep convolutional neural network;Instruments;Music;Speech recognition;Convolution;Neural networks;Speech;Machine learning;Convolutional neural networks;deep learning;instrument recognition;multi-layer neural network;music information retrieval},
doi={10.1109/TASLP.2016.2632307},
ISSN={2329-9290},
month={Jan},}

% [36]
@inproceedings{Lostanlen2016,
author       = {Vincent Lonstanlen and Carmine-Emanuele Cella},
title        = {Deep convolutional networks on the pitch spiral for musical instrument recognition},
booktitle={Proceedings of the 17th International Society for Music Information Retrieval Conference},
year         = 2016,
pages        = {612-618},
publisher    = {ISMIR},
address      = {New York, USA},
month        = August,
venue        = {New York, USA}
}

% [37]
@inproceedings{siddharth2018,
author       = {Siddharth Gururani and Cameron Summers and Alexander Lerch},
title        = {Instrument Activity Detection in Polyphonic Music using Deep Neural Networks},
booktitle    = {Proceedings of the 19th International Society for Music Information Retrieval Conference},
year         = 2018,
pages        = {569-576},
publisher    = {ISMIR},
address      = {Paris, France},
month        = sep,
venue        = {Paris, France},
doi          = {10.5281/zenodo.1492479},
}


% [38]
@inproceedings{schlutter2018,
author       = {Jan Schl\"{u}tter and Bernhard Lehner},
title        = {Zero Mean Convolutions for Level-Invariant Singing Voice Detection},
booktitle    = {Proceedings of the 19th International Society for Music Information Retrieval Conference},
year         = 2018,
pages        = {321-326},
publisher    = {ISMIR},
address      = {Paris, France},
month        = sep,
venue        = {Paris, France}
}

%  [39]
@INPROCEEDINGS{Delikaris2017, 
author={S. {Delikaris-Manias} and D. {Pavlidi} and A. {Mouchtaris} and V. {Pulkki}}, 
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={{DOA} estimation with histogram analysis of spatially constrained active intensity vectors}, 
year={2017}, 
pages={526-530}, 
keywords={array signal processing;audio signal processing;direction-of-arrival estimation;microphone arrays;vectors;direction of arrival estimator;DOA estimation;spatially constrained active intensity vectors;AIV;sound field;microphone array processing;beamforming operations;sound sources;spatially-constrained regions;SCR;1D histogram processing;mobile microphone array;signal-to-noise ratios;Direction-of-arrival estimation;Histograms;Estimation;Microphone arrays;Array signal processing;Mobile communication;direction of arrival;higher order active intensity vector;multiple sound sources;microphone arrays}, 
doi={10.1109/ICASSP.2017.7952211}, 
ISSN={2379-190X}, 
month={March},}

%  [40]
@ARTICLE{Habets2019, 
author={S. {Chakrabarty} and E. A. P. {Habets}}, 
journal={IEEE Journal of Selected Topics in Signal Processing}, 
title={Multi-Speaker {DOA} Estimation Using Deep Convolutional Networks Trained With Noise Signals}, 
year={2019}, 
volume={13}, 
number={1}, 
pages={8-21}, 
keywords={acoustic noise;acoustic signal processing;convolutional neural nets;direction-of-arrival estimation;Fourier transforms;learning (artificial intelligence);microphones;pattern classification;speech processing;time-frequency analysis;DOA label;separate binary classification problem;received microphone signals;CNN;disjoint speaker activity;synthesized noise signals;acoustic impulse responses;DOA estimation approach;unseen acoustic conditions;multispeaker DOA estimation;deep convolutional networks trained;supervised learning-based methods;adverse acoustic environments;convolutional neural network;multiple speakers;multiclass multilabel classification problem;acoustic conditions;Direction-of-arrival estimation;Estimation;Acoustics;Microphone arrays;Task analysis;Training data;Source localization;multiple speakers;convolutional neural networks}, 
doi={10.1109/JSTSP.2019.2901664}, 
ISSN={1932-4553}, 
month={March},}

% [41]
@ARTICLE{Li2017,
author={X. {Li} and L. {Girin} and R. {Horaud} and S. {Gannot}},
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
title={Multiple-Speaker Localization Based on Direct-Path Features and Likelihood Maximization With Spatial Sparsity Regularization},
year={2017},
volume={25},
number={10},
pages={1997-2012},
keywords={audio recording;entropy;Fourier transforms;Gaussian processes;maximum likelihood estimation;mixture models;reverberation;speaker recognition;transfer functions;multiple-speaker localization;direct-path features;likelihood maximization;spatial sparsity regularization;reverberant environment;binaural recordings;acoustic scene;complex-valued Gaussian mixture model;CGMM-based objective function;complex-valued binaural features;entropy-based penalty term;CGMM component weights;initial candidate source locations;direct-path relative transfer function;robust binaural features;single-source localization;interchannel information encoding;sound propagation;reverberations;DP-RTF estimation;short-time Fourier transform domain;consistency test;robotic head;multisource localization method;Speech;Position measurement;Transfer functions;Reverberation;Microphones;Robustness;Candidate-based GMM;direct-path RTF;entropy penalty;multiple-speaker localization},
doi={10.1109/TASLP.2017.2740001},
ISSN={2329-9290},
month={Oct},}

%  [42]
@article{GRONDIN2019,
title = "Lightweight and optimized sound source localization and tracking methods for open and closed microphone array configurations",
journal = "Robotics and Autonomous Systems",
volume = "113",
pages = "63 - 80",
year = "2019",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2019.01.002",
author = "François Grondin and François Michaud",
keywords = "Sound source localization, Sound source tracking, Microphone array, Online processing, Embedded system, Mobile robot, Robot audition",
abstract = "Human–robot interaction in natural settings requires filtering out the different sources of sounds from the environment. Such ability usually involves the use of microphone arrays to localize, track and separate sound sources online. Multi-microphone signal processing techniques can improve robustness to noise but the processing cost increases with the number of microphones used, limiting response time and widespread use on different types of mobile robots. Since sound source localization methods are the most expensive in terms of computing resources as they involve scanning a large 3D space, minimizing the amount of computations required would facilitate their implementation and use on robots. The robot’s shape also brings constraints on the microphone array geometry and configurations. In addition, sound source localization methods usually return noisy features that need to be smoothed and filtered by tracking the sound sources. This paper presents a novel sound source localization method, called SRP-PHAT-HSDA, that scans space with coarse and fine resolution grids to reduce the number of memory lookups. A microphone directivity model is used to reduce the number of directions to scan and ignore non significant pairs of microphones. A configuration method is also introduced to automatically set parameters that are normally empirically tuned according to the shape of the microphone array. For sound source tracking, this paper presents a modified 3D Kalman (M3K) method capable of simultaneously tracking in 3D the directions of sound sources. Using a 16-microphone array and low cost hardware, results show that SRP-PHAT-HSDA and M3K perform at least as well as other sound source localization and tracking methods while using up to 4 and 30 times less computing resources respectively."
}

% [43]
@ARTICLE{Yook2016, 
author={D. {Yook} and T. {Lee} and Y. {Cho}}, 
journal={IEEE Transactions on Cybernetics}, 
title={Fast Sound Source Localization Using Two-Level Search Space Clustering}, 
year={2016}, 
volume={46}, 
number={1}, 
pages={20-26}, 
keywords={acoustic signal processing;microphone arrays;search problems;sound source localization;search space clustering;steered response power phase transform;large-scale microphone array systems;Microphones;Search methods;Arrays;Power generation;Table lookup;Accuracy;Computational efficiency;Search space clustering (SSC);sound source localization (SSL);steered response power phase transform (SRP-PHAT);Search space clustering (SSC);sound source localization (SSL);steered response power phase transform (SRP-PHAT)}, 
doi={10.1109/TCYB.2015.2391252}, 
ISSN={2168-2267}, 
month={Jan},}

% [44]
@ARTICLE{Pavlidi2013, 
author={D. {Pavlidi} and A. {Griffin} and M. {Puigt} and A. {Mouchtaris}}, 
journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
title={Real-Time Multiple Sound Source Localization and Counting Using a Circular Microphone Array}, 
year={2013}, 
volume={21}, 
number={10}, 
pages={2193-2206}, 
keywords={audio signal processing;direction-of-arrival estimation;microphone arrays;time-frequency analysis;real-time multiple sound source localization method;real-time multiple sound source counting method;circular microphone array;relaxed sparsity constraint;uniform circular microphone array;linear array;microphone array topology;time-frequency zone detection;TF component;direction of arrival;DOA estimation;matching pursuit-based approach;computational complexity;Direction-of-arrival estimation;Microphones;Estimation;Arrays;Time-frequency analysis;Matching pursuit algorithms;Real-time systems;Direction of arrival estimation;matching pursuit;microphone array signal processing;multiple source localization;real-time localization;source counting;sparse component analysis}, 
doi={10.1109/TASL.2013.2272524}, 
ISSN={1558-7916}, 
month={Oct},}

% [45]
@INPROCEEDINGS{Vecchiotti:Binaural-Endtoend-2019,
author={P. {Vecchiotti} and N. {Ma} and S. {Squartini} and G. J. {Brown}},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={End-to-end Binaural Sound Localisation from the Raw Waveform},
year={2019},
volume={},
number={},
pages={451-455},
ISSN={2379-190X},
month={May},}

%  [46]
@ARTICLE{Luo2018, 
author={Y. {Luo} and Z. {Chen} and N. {Mesgarani}}, 
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
title={Speaker-Independent Speech Separation With Deep Attractor Network}, 
year={2018}, 
volume={26}, 
number={4}, 
pages={787-796}, 
keywords={learning (artificial intelligence);neural nets;signal reconstruction;signal representation;source separation;speech processing;speech recognition;speaker mixtures;deep learning methods;speaker-independent speech separation;deep attractor network;masker speakers;output dimension problem;deep learning framework;time-frequency representation;high-dimensional embedding space;reference point;time-frequency embeddings;corresponding attractor point;time-frequency assignment;standard signal reconstruction;Speech;Time-frequency analysis;Spectrogram;Machine learning;Speech processing;Neural networks;Source separation;multi-talker;deep clustering;attractor network}, 
doi={10.1109/TASLP.2018.2795749}, 
ISSN={2329-9290}, 
month={April},}

% [47]
@INPROCEEDINGS{Wang2018, 
author={Z. {Wang} and J. {Le Roux} and J. R. {Hershey}}, 
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Multi-Channel Deep Clustering: Discriminative Spectral and Spatial Embeddings for Speaker-Independent Speech Separation}, 
year={2018},
pages={1-5}, 
keywords={microphone arrays;speech processing;spatial embeddings;speaker-independent speech separation;deep clustering algorithm;cocktail party problem;deep clustering framework;inter-microphone phase patterns;spatial-ized version;multichannel deep clustering;discriminative spectral embeddings;microphones;microphone array;Clustering algorithms;Training;Array signal processing;Delay effects;Time-frequency analysis;Microphone arrays;deep clustering;spatial clustering;deep learning;cocktail party problem;speaker-independent speech separation}, 
doi={10.1109/ICASSP.2018.8461639}, 
ISSN={2379-190X}, 
month={April},}

% [48]
@INPROCEEDINGS{Naithani2017,
author={G. {Naithani} and T. {Barker} and G. {Parascandolo} and L. {Bramsl⊘w} and N. H. {Pontoppidan} and T. {Virtanen}},
booktitle={Proc. of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)},
title={Low latency sound source separation using convolutional recurrent neural networks},
year={2017},
pages={71-75},
keywords={learning (artificial intelligence);matrix decomposition;recurrent neural nets;signal reconstruction;source separation;low latency sound source separation;deep neural networks;monaural sound source separation;convolutional recurrent neural network architecture;low algorithmic delay;slightly better performance;separation performance metrics;short term objective intelligibility scores;training data;CRNN architecture;feedforward DNN;long short-term memory networks;LSTM networks;time 10.0 ms;Convolution;Neural networks;Training data;Training;Source separation;Time-frequency analysis;Source Separation;Low-latency;Deep Neural Networks;Convolutional Recurrent Neural Networks},
doi={10.1109/WASPAA.2017.8169997},
ISSN={1947-1629},
month={Oct},}

% [49]
@INPROCEEDINGS{Sunohara2017,
author={M. {Sunohara} and C. {Haruta} and N. {Ono}},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Low-latency real-time blind source separation for hearing aids based on time-domain implementation of online independent vector analysis with truncation of non-causal components},
year={2017},
pages={216-220},
keywords={blind source separation;FIR filters;hearing aids;medical signal processing;time-domain analysis;blind source separation;hearing aids;time-domain implementation;online auxiliary-function-based independent vector analysis;AuxIVA;audio applications;frequency-domain BSS methods;separation filters;multiple FIR filters;Delays;Frequency-domain analysis;Hearing aids;Finite impulse response filters;Microphones;Real-time systems;Time-domain analysis;blind source separation;independent vector analysis;hearing aids;algorithmic delay;causality},
doi={10.1109/ICASSP.2017.7952149},
ISSN={2379-190X},
month={March},}


%  [50]
@INPROCEEDINGS{Luo2018_ICassp,
author={Y. {Luo} and N. {Mesgarani}},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={{TaSNet}: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation},
year={2018},
pages={696-700},
keywords={decoding;source separation;speech coding;time-frequency analysis;Time-domain Audio Separation Network;TasNet;frequency decomposition step;noncausal speech separation algorithms;effective speech separation;time-frequency representation;time-frequency decomposition results;single-channel speech separation;robust speech processing;deep learning systems;frequency resolution;multi-talker environments;real-time short latency applications;nonnegative encoder outputs;source masks;Training;Decoding;Time-domain analysis;Time-frequency analysis;Convolution;Real-time systems;Estimation;Source separation;single channel;raw waveform;deep learning},
doi={10.1109/ICASSP.2018.8462116},
ISSN={2379-190X},
month={April},}

%  [51]
@INPROCEEDINGS{Chua2016,
author={J. {Chua} and G. {Wang} and W. B. {Kleijn}},
booktitle={Proc. of IEEE International Workshop on Acoustic Signal Enhancement (IWAENC)},
title={Convolutive blind source separation with low latency},
year={2016},
pages={1-5},
keywords={blind source separation;convolution;filtering theory;Fourier transforms;convolutive blind source separation method;low latency;crossband filtering;short-time Fourier transform domain;STFT domain;window length;TF-domain BSS algorithms;Time-frequency analysis;Indexes;Time-domain analysis;Microphones;Blind source separation;Estimation;Blind source separation;crossband filtering;low latency},
doi={10.1109/IWAENC.2016.7602895},
ISSN={},
month={Sep.},}

% [52]
@ARTICLE{Rafii2018, 
author={Z. {Rafii} and A. {Liutkus} and F. {St{\"o}ter} and S. I. {Mimilakis} and D. {FitzGerald} and B. {Pardo}}, 
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
title={An Overview of Lead and Accompaniment Separation in Music}, 
year={2018}, 
volume={26}, 
number={8}, 
pages={1307-1335}, 
keywords={audio signal processing;learning (artificial intelligence);music;source separation;accompaniment separation;popular music;lead component;automatic karaoke;remixing;source separation yields;particular complexity;musical structures;relevant prior knowledge;musicology;popular topic;model-based methods;lead signal;data-centered approaches;particular difficulty;lead separation systems;music separation;deep learning;Speech;Lead;Speech processing;Spectrogram;Music;Time-frequency analysis;Source separation;Source separation;music;accompaniment;lead;overview}, 
doi={10.1109/TASLP.2018.2825440}, 
ISSN={2329-9290}, 
month={Aug},}

%  [53]
@InProceedings{Stoter2018,
author="St{\"o}ter, Fabian-Robert and Liutkus, Antoine and Ito, Nobutaka",
editor="Deville, Yannick
and Gannot, Sharon
and Mason, Russell
and Plumbley, Mark D.
and Ward, Dominic",
title="The 2018 Signal Separation Evaluation Campaign",
booktitle="Latent Variable Analysis and Signal Separation",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="293--305",
abstract="This paper reports the organization and results for the 2018 community-based Signal Separation Evaluation Campaign (SiSEC 2018). This year's edition was focused on audio and pursued the effort towards scaling up and making it easier to prototype audio separation software in an era of machine-learning based systems. For this purpose, we prepared a new music separation database: MUSDB18, featuring close to 10 h of audio. Additionally, open-source software was released to automatically load, process and report performance on MUSDB18. Furthermore, a new official Python version for the BSS Eval toolbox was released, along with reference implementations for three oracle separation methods: ideal binary mask, ideal ratio mask, and multichannel Wiener filter. We finally report the results obtained by the participants.",
isbn="978-3-319-93764-9"
}

%  [54]
@Article{durrieu2011,
Title                    = {A Musically Motivated Mid-Level Representation for Pitch Estimation and Musical Audio Source Separation},
Author                   = {Durrieu, J.-L. and David, B. and Richard, G.},
Journal                  = {Selected Topics in Signal Processing, IEEE Journal of},
Year                     = {2011},
Month                    = {Oct. },
Number                   = {6},
Pages                    = {1180 -1191},
Volume                   = {5},
Doi                      = {10.1109/JSTSP.2011.2158801},
ISSN                     = {1932-4553},
Owner                    = {papa},
Timestamp                = {2012.05.14}
}

% [55]
@inproceedings{Uhlich2017,
author = {Uhlich, Stefan and Porcu, Marcello and Giron, Franck and Enenkl, Michael and Kemp, Thomas and Takahashi, Naoya and Mitsufuji, Yuki},
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
year = {2017},
title = {Improving music source separation based on deep neural networks through data augmentation and network blending},
doi = {10.1109/ICASSP.2017.7952158}
}

% [56]
@ARTICLE{Samarasinghe2016,
author={P. N. {Samarasinghe} and W. {Zhang} and T. D. {Abhayapala}},
journal={IEEE Signal Processing Magazine},
title={Recent Advances in Active Noise Control Inside Automobile Cabins: Toward quieter cars},
year={2016},
volume={33},
number={6},
pages={61-73},
keywords={active noise control;automobiles;ANC techniques;active noise control;automobile cabins;noise reduction levels;Automotive engineering;Acoustics;Noise measurement;Control systems;Vibrations;Feedforward neural networks;Smart devices;Intelligent vehicles},
doi={10.1109/MSP.2016.2601942},
ISSN={1053-5888},
month={Nov},}

% [57]
@article{PAPINI2017102,
title = "Hybrid approach to noise control of industrial exhaust systems",
journal = "Applied Acoustics",
volume = "125",
pages = "102 - 112",
year = "2017",
issn = "0003-682X",
doi = "https://doi.org/10.1016/j.apacoust.2017.03.017",
author = "Guilherme S. Papini and Ricardo L.U.F. Pinto and Eduardo B. Medeiros and Filipe B.G. Coelho",
keywords = "Optimal noise control design, Resistive silencers, Reactive silencers, Hybrid approach for noise control, Hybrid approach",
abstract = "This study aims to present a procedure for optimal design of hybrid noise control systems for exhaust ducts from large fans and high radiated sound power, in an industrial environment. Reactive, resistive or hybrid silencers are considered as alternatives with the possibility of adding a supplementary active noise control system to mitigate remaining noise of multiple low frequencies, which are difficult to be attenuated by passive means. Algorithms are proposed for the implementation of optimal hybrid passive/active design and the main theoretical and practical fundamentals involved are showed. Two effective solutions for a real industrial exhaust system are presented as examples of applications of hybrid optimal design procedure proposed. The first one uses of Helmholtz resonators (to attenuate lower frequencies) and resistive silencer parallel lamellae (to lower the midrange and high frequencies). The second one adopts resistive silencer of parallel lamellae and a forward active noise control system. This is a hybrid approach that is not treated in this manner in the available literature, especially in the text books of applied acoustics engineering."
}

% [58]
@ARTICLE{Zhang2018, 
author={J. {Zhang} and T. D. {Abhayapala} and W. {Zhang} and P. N. {Samarasinghe} and S. {Jiang}}, 
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
title={Active Noise Control Over Space: A Wave Domain Approach}, 
year={2018}, 
volume={26}, 
number={4}, 
pages={774-786}, 
keywords={acoustic signal processing;active noise control;loudspeakers;minimisation;signal denoising;wave domain approach;fundamental problem;acoustic signal processing;wave-domain adaptive algorithms;secondary source driving signals;primary noise field;control region;wave-domain active noise control algorithms;minimization problems;wave-domain residual signal coefficients;loudspeaker weights;wave-domain secondary source coefficients;noise cancellation performance;noise reduction level;acoustic potential energy reduction level;Loudspeakers;Harmonic analysis;Adaptive algorithms;Potential energy;Microphone arrays;Noise cancellation;Active noise control (ANC);wave domain;multichannel;spatial noise;reverberant room}, 
doi={10.1109/TASLP.2018.2795756}, 
ISSN={2329-9290}, 
month={April},}

% [59]
@inproceedings{Lu2013SpeechEB,
title={Speech enhancement based on deep denoising autoencoder},
author={Xugang Lu and Yu Tsao and Shigeki Matsuda and Chiori Hori},
booktitle={Proc. of Interspeech},
year={2013}
}

% [60]
@ARTICLE{Xu2015,
author={Y. {Xu} and J. {Du} and L. {Dai} and C. {Lee}},
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
title={A Regression Approach to Speech Enhancement Based on Deep Neural Networks},
year={2015},
volume={23},
number={1},
pages={7-19},
keywords={least mean squares methods;neural nets;regression analysis;speech enhancement;speech enhancement;deep neural networks;supervised method;DNN architecture;nonlinear regression function;global variance equalization;over-smoothing problem;MMSE based technique;noisy speech data;musical artifact;Speech;Noise;Speech enhancement;Training;Noise measurement;IEEE transactions;Deep neural networks (DNNs);dropout;global variance equalization;noise aware training;noise reduction;nonstationary noise;speech enhancement},
doi={10.1109/TASLP.2014.2364452},
ISSN={2329-9290},
month={Jan},}


% [61]
@INPROCEEDINGS{PascualBS17,
author    = {Santiago Pascual andAntonio Bonafonte and Joan Serr{\`{a}}},
title     = {{SEGAN:} Speech Enhancement Generative Adversarial Network},
pages     = {3642-3646}, 
booktitle = {Proc. of Interspeech},
year      = {2017},
month={August}
}

% [62]
@InProceedings{weniger2015,
author="Weninger, Felix
and Erdogan, Hakan
and Watanabe, Shinji
and Vincent, Emmanuel
and Le Roux, Jonathan
and Hershey, John R.
and Schuller, Bj{\"o}rn",
editor="Vincent, Emmanuel
and Yeredor, Arie
and Koldovsk{\'y}, Zbyn{\v{e}}k
and Tichavsk{\'y}, Petr",
title="Speech Enhancement with {LSTM} Recurrent Neural Networks and its Application to Noise-Robust {ASR}",
booktitle="Latent Variable Analysis and Signal Separation",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="91--99",
abstract="We evaluate some recent developments in recurrent neural network (RNN) based speech enhancement in the light of noise-robust automatic speech recognition (ASR). The proposed framework is based on Long Short-Term Memory (LSTM) RNNs which are discriminatively trained according to an optimal speech reconstruction objective. We demonstrate that LSTM speech enhancement, even when used `na{\"i}vely' as front-end processing, delivers competitive results on the CHiME-2 speech recognition task. Furthermore, simple, feature-level fusion based extensions to the framework are proposed to improve the integration with the ASR back-end. These yield a best result of 13.76 {\%} average word error rate, which is, to our knowledge, the best score to date.",
isbn="978-3-319-22482-4"
}

% [63]
@conference{wierstorf2017perceptual,
title = {Perceptual Evaluation of Source Separation for Remixing Music},
author = {Wierstorf, Hagen and Ward, Dominic and Mason, Russell and Grais, Emad M. and Hummersone, Chris and Plumbley, Mark D.},
booktitle = {Proc. of Audio Engineering Society Convention 143},
month = {Oct},
year = {2017},
}

% [64]
@article{Pons2016,
author = {Pons,Jordi  and Janer,Jordi  and Rode,Thilo  and Nogueira,Waldo },
title = {Remixing music using source separation algorithms to improve the musical experience of cochlear implant users},
journal = {The Journal of the Acoustical Society of America},
volume = {140},
number = {6},
pages = {4338-4349},
year = {2016},
doi = {10.1121/1.4971424},
}


% [65]
@INPROCEEDINGS{qkong_aed,
author={Q. Kong and Y. Xu and W. Wang and M. D. Plumbley},
booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={A JOINT SEPARATION-CLASSIFICATION MODEL FOR SOUND EVENT DETECTION OF
WEAKLY LABELLED DATA},
year={2018},
pages={},
month={March},}


% [66]
@INPROCEEDINGS{Neumann2019, 
author={T. v. {Neumann} and K. {Kinoshita} and M. {Delcroix} and S. {Araki} and T. {Nakatani} and R. {Haeb-Umbach}}, 
booktitle={Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={All-neural Online Source Separation, Counting, and Diarization for Meeting Analysis}, 
year={2019}, 
pages={91-95}, 
keywords={Artificial neural networks;Source separation;Speech recognition;Estimation;Task analysis;Indexes;Blind source separation;neural network;meeting diarization;online processing;source counting}, 
doi={10.1109/ICASSP.2019.8682572}, 
ISSN={2379-190X}, 
month={May},}

% [67]
@inproceedings{Gharib2018,
author = "Gharib, Shayan and Drossos, Konstantinos and Cakir, Emre and Serdyuk, Dmitriy and Virtanen, Tuomas",
title = "Unsupervised adversarial domain adaptation for acoustic scene classification",
year = "2018",
month = "November",
pages = "138--142",
keywords = "Adversarial domain adaptation, acoustic scene classification",
booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop (DCASE)"
}

% [68]
@inproceedings{Mesaros:2018:DCASE,
 author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
 title = {A~Multi-Device Dataset for Urban Acoustic Scene Classification},
 booktitle = {Proceedings of the~Detection and~Classification of Acoustic Scenes and Events Workshop},
 year = {2018},
 address = {Surrey, UK}
}







